{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9477189,"sourceType":"datasetVersion","datasetId":5764119},{"sourceId":9478835,"sourceType":"datasetVersion","datasetId":5765381},{"sourceId":9505903,"sourceType":"datasetVersion","datasetId":5785692},{"sourceId":9747083,"sourceType":"datasetVersion","datasetId":5964915},{"sourceId":9766063,"sourceType":"datasetVersion","datasetId":5700992}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install evaluate","metadata":{"_uuid":"2d57267c-456b-4611-bf7a-86007ea7f0ab","_cell_guid":"96620005-1fea-412c-8601-116c6cad285b","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-01T04:42:18.783149Z","iopub.execute_input":"2024-11-01T04:42:18.783470Z","iopub.status.idle":"2024-11-01T04:42:32.058797Z","shell.execute_reply.started":"2024-11-01T04:42:18.783437Z","shell.execute_reply":"2024-11-01T04:42:32.057648Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# !pip install -q huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2024-11-01T04:42:32.060720Z","iopub.execute_input":"2024-11-01T04:42:32.061045Z","iopub.status.idle":"2024-11-01T04:42:32.065401Z","shell.execute_reply.started":"2024-11-01T04:42:32.061008Z","shell.execute_reply":"2024-11-01T04:42:32.064527Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from datasets import Dataset\nimport pandas as pd\nimport evaluate\nimport numpy as np\nfrom transformers import (\n    AutoModelForSequenceClassification, TrainingArguments, Trainer,\n    DataCollatorWithPadding, AutoTokenizer, set_seed, EarlyStoppingCallback\n)\nimport os\nimport argparse\nimport torch\n\ndef preprocess_function(examples, **fn_kwargs):\n    return fn_kwargs['tokenizer'](examples[\"text\"], truncation=True,\n                                  max_length=512\n                                 )\n\ndef get_data(train_path, dev_path, test_path, random_seed):\n    \"\"\"\n    function to read dataframe with columns\n    \"\"\"\n\n    train_df = pd.read_json(train_path, lines=True)\n    val_df = pd.read_json(dev_path, lines=True)\n    test_df = pd.read_json(test_path, lines=True)\n    \n    return train_df, val_df, test_df\n\ndef compute_metrics(eval_pred):\n\n    f1_metric = evaluate.load(\"f1\")\n\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    \n    results = {}\n    results.update(f1_metric.compute(predictions=predictions, references = labels, average=\"macro\"))\n\n    return results\n\n\ndef fine_tune(train_df, valid_df, checkpoints_path, id2label, label2id, model, train_tem_args):\n    \n    torch.cuda.empty_cache()\n        \n    # pandas dataframe to huggingface Dataset\n    train_dataset = Dataset.from_pandas(train_df)\n    valid_dataset = Dataset.from_pandas(valid_df)\n    \n    cache_dir = \"./huggingface_cache\"  # Directory to store the downloaded model\n    \n    # get tokenizer and model from huggingface\n    tokenizer = AutoTokenizer.from_pretrained(model)    \n#     tokenizer = AutoTokenizer.from_pretrained(model, padding=\"longest\", truncation=True)\n    model = AutoModelForSequenceClassification.from_pretrained(\n       model, num_labels=len(label2id), id2label=id2label, label2id=label2id,\n    )\n    \n    for param in model.parameters(): param.data = param.data.contiguous()\n    \n    # tokenize data for train/valid\n    tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n    tokenized_valid_dataset = valid_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n    \n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n#     data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pin_memory=True)\n    \n\n    # Calculate eval_steps to evaluate 3 times per epoch\n    total_train_samples = len(train_dataset)\n    batch_size = train_tem_args['train_batch']\n    steps_per_epoch = total_train_samples // batch_size\n    eval_steps = steps_per_epoch // 4  # Evaluate 3 times per epoch\n\n    training_args = TrainingArguments(\n        output_dir=checkpoints_path,\n        learning_rate=train_tem_args['lr'],\n        per_device_train_batch_size=train_tem_args['train_batch'],\n        per_device_eval_batch_size=train_tem_args['val_batch'],\n        num_train_epochs=train_tem_args['epochs'],\n        weight_decay=train_tem_args['weight_decay'],\n        save_strategy=\"steps\",  # Save based on steps\n        logging_steps=eval_steps,  # Log every eval_steps\n        evaluation_strategy=\"steps\",\n        save_total_limit=2,  # Save only the last 2 checkpoints\n        save_steps=eval_steps,  # Save model every eval_steps\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n    )\n    \n    early_stopping = EarlyStoppingCallback(\n        early_stopping_patience=5,  # Stop if no improvement after 3 evaluations\n        early_stopping_threshold=0.001  # Minimum improvement threshold\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train_dataset,\n        eval_dataset=tokenized_valid_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n        callbacks=[early_stopping],  # Add early stopping callback\n    )\n\n    trainer.train()\n\n    # save best model\n    best_model_path = os.path.join(checkpoints_path, 'best')\n    \n    if not os.path.exists(best_model_path):\n        os.makedirs(best_model_path)\n    \n    trainer.save_model(best_model_path)\n\n\ndef test(test_df, model_path, id2label, label2id):\n    \n    # load tokenizer from saved model \n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    # load best model\n    model = AutoModelForSequenceClassification.from_pretrained(\n       model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id\n    )\n            \n    test_dataset = Dataset.from_pandas(test_df)\n\n    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    # create Trainer\n    trainer = Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n    # get logits from predictions and evaluate results using classification report\n    predictions = trainer.predict(tokenized_test_dataset)\n    preds = np.argmax(predictions.predictions, axis=-1)\n#     metric = evaluate.load(\"bstrai/classification_report\")\n#     results = metric.compute(predictions=preds, references=predictions.label_ids)\n    \n    # return dictionary of classification report\n    return preds","metadata":{"_uuid":"7d688dc3-5db0-45f6-8694-38c3b95a974f","_cell_guid":"3a998121-caa7-406a-85bc-9e20a19a72f9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-01T04:42:32.066651Z","iopub.execute_input":"2024-11-01T04:42:32.066932Z","iopub.status.idle":"2024-11-01T04:42:51.830305Z","shell.execute_reply.started":"2024-11-01T04:42:32.066902Z","shell.execute_reply":"2024-11-01T04:42:51.829519Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Training English","metadata":{"_uuid":"61e947e8-6f42-4225-96d8-95ba9e85cef4","_cell_guid":"07553d26-d417-4acb-af1d-3117cad045f9","trusted":true}},{"cell_type":"code","source":"!pip install -q --upgrade transformers\n!pip install -q wandb==0.16.6","metadata":{"execution":{"iopub.status.busy":"2024-11-01T04:42:51.831607Z","iopub.execute_input":"2024-11-01T04:42:51.832723Z","iopub.status.idle":"2024-11-01T04:43:39.222183Z","shell.execute_reply.started":"2024-11-01T04:42:51.832678Z","shell.execute_reply":"2024-11-01T04:43:39.221006Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\n\ndef custom_sample_balanced_data(df, lang_column, label_column, lang_limits=None, random_state=42):\n    \n    # Initialize an empty DataFrame to store the final balanced data\n    balanced_df = pd.DataFrame()\n\n    # Loop through each unique language\n    for lang in df[lang_column].unique():\n        # Filter the DataFrame for the current language\n        lang_df = df[df[lang_column] == lang]\n        \n        # If the language is in the lang_limits dictionary, perform balanced sampling\n        if lang in lang_limits:\n            # Split by label to balance the labels (0 and 1)\n            label_0 = lang_df[lang_df[label_column] == 0]\n            label_1 = lang_df[lang_df[label_column] == 1]\n            \n            # Determine the maximum possible samples per label (50:50 ratio)\n            n_samples_per_label = min(len(label_0), len(label_1), lang_limits[lang] // 2)\n            \n            if n_samples_per_label > 0:\n                # Sample the rows for each label with equal number of rows\n                sampled_label_0 = label_0.sample(n=n_samples_per_label, random_state=random_state)\n                sampled_label_1 = label_1.sample(n=n_samples_per_label, random_state=random_state)\n                \n                # Combine the sampled data for the current language\n                balanced_lang_df = pd.concat([sampled_label_0, sampled_label_1], ignore_index=True)\n                \n                # Append to the final DataFrame\n                balanced_df = pd.concat([balanced_df, balanced_lang_df], ignore_index=True)\n        else:\n            # For languages not in lang_limits, keep all rows\n            balanced_df = pd.concat([balanced_df, lang_df], ignore_index=True)\n\n    # Shuffle the final DataFrame to mix the rows\n    balanced_df = balanced_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n\n    return balanced_df\n","metadata":{"execution":{"iopub.status.busy":"2024-11-01T04:43:39.225960Z","iopub.execute_input":"2024-11-01T04:43:39.226789Z","iopub.status.idle":"2024-11-01T04:43:39.237225Z","shell.execute_reply.started":"2024-11-01T04:43:39.226735Z","shell.execute_reply":"2024-11-01T04:43:39.236400Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# # Initialize Weights & Biases (W&B) in disabled mode.\n\nimport wandb\nwandb.init(mode=\"disabled\")\n\n!rm -r /kaggle/working/\n!rm -rf ~/.cache/huggingface\n!rm -rf ~/.cache/\n\nprint(\"Training started----------------------------------------------\")\n\ntorch.cuda.empty_cache()","metadata":{"_uuid":"3d93729c-a6eb-4a0c-b242-d04096a34fc8","_cell_guid":"2e0ba763-228c-4c97-9c9f-c7730bda3a2e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-01T04:43:39.238349Z","iopub.execute_input":"2024-11-01T04:43:39.238624Z","iopub.status.idle":"2024-11-01T04:43:43.848184Z","shell.execute_reply.started":"2024-11-01T04:43:39.238594Z","shell.execute_reply":"2024-11-01T04:43:43.846904Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"rm: cannot remove '/kaggle/working/': Device or resource busy\nTraining started----------------------------------------------\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Set paths and parameters\ntrain_path = '/kaggle/input/coling-25-task-1/coling25task1/multilingual_train.jsonl' \ndev_path = '/kaggle/input/coling-25-task-1/coling25task1/multilingual_dev.jsonl'    \ntest_path = '/kaggle/input/coling-25-task-1/coling25task1/multilingual_devtest_text_id_only.jsonl'   \ncheckpoints_path = '/kaggle/working/checkpoints'\n\n# train_path = '/kaggle/input/short-mul-lang-coling/multilingual_train_short.jsonl' \n# dev_path = '/kaggle/input/short-mul-lang-coling/multilingual_dev_short.jsonl' \n\nmodel = \"xlm-roberta-base\"\nmodel_name = '/kaggle/input/mul-ensemble-coling-final/mullang_xlm-roberta-base(1)'\n\nprediction_path = '/kaggle/working/subtask_b_pred.jsonl'\nrandom_seed = 42\n\n# Set logging and seed\n# logging.basicConfig(level=logging.INFO)\nset_seed(random_seed)\n\ntrain_tem_args = {\n    'epochs': 3,\n    'lr': 1e-5,\n    'weight_decay': 0.01,\n    'train_batch': 4,\n    'val_batch': 16,\n}\n\n\nid2label = {0: \"human\", 1: \"machine\"}\nlabel2id = {\"human\": 0, \"machine\": 1}\n\n\n#get data for train/dev/test sets\ntrain_df, valid_df, test_df = get_data(train_path, dev_path, test_path, random_seed)\n\nlang_limits = {'en': 62000, 'zh': 30000}\n# Call the function\ntrain_df = custom_sample_balanced_data(train_df, lang_column='lang', label_column='label', lang_limits=lang_limits)\n\nlang_limits = {'en': 20000, 'zh': 14000}\n# Call the function\nvalid_df = custom_sample_balanced_data(valid_df, lang_column='lang', label_column='label', lang_limits=lang_limits)\n\n\nreduced_val = 1\n# reduced_val = 0.001\n# Sample 10% of the DataFrame\ntrain_df = train_df.sample(n=int(len(train_df) * reduced_val), random_state=42)\nvalid_df = valid_df.sample(n=int(len(valid_df) * reduced_val), random_state=42)\n\nprint(train_df['lang'].value_counts(),valid_df['lang'].value_counts())\n\n\nprint(\"data loaded--------------\")\n\n# train detector model\nfine_tune(train_df, valid_df, f\"{model}/{random_seed}\", id2label, label2id, model_name, train_tem_args)\n# fine_tune(train_df, valid_df, f\"testing\", id2label, label2id, model, train_tem_args)\n\nprint(\"Training Done --------------\")","metadata":{"_uuid":"210e3f7e-1d6e-4f81-8e35-cb217f0efd15","_cell_guid":"57cd5d2d-8910-47c9-9b76-8ce1453c5ef6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-01T04:43:43.849808Z","iopub.execute_input":"2024-11-01T04:43:43.851008Z"},"trusted":true},"outputs":[{"name":"stdout","text":"lang\nen    62000\nzh    30000\nbg     8091\nde     4693\nit     4174\nid     3976\nur     3761\nar     2114\nru     1314\nName: count, dtype: int64 lang\nen    20000\nzh    13498\nbg     3489\nde     2059\nit     1843\nid     1803\nur     1573\nar      906\nru      600\nName: count, dtype: int64\ndata loaded--------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/120123 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0821fc6f3d574fa2b08d75ed3ddc77bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45771 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aab8ca74bcc4b00aa827ce96c9d2086"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  # expand paths, if not os.makedirs(\"~/bar\") will make directory\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='650' max='90093' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  650/90093 02:41 < 6:11:31, 4.01 it/s, Epoch 0.02/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define your variables\ndir_to_zip = f\"{model}/{random_seed}/best/\"\n\n# Define the name of the output zip file\nlast_word = model.split('/')[-1]\noutput_zip = f\"mullang_{last_word}.zip\"\n\n# Create a zip file from the directory\nshutil.make_archive(output_zip.replace('.zip', ''), 'zip', dir_to_zip)\n\nprint(f\"Zipped contents of {dir_to_zip} into {output_zip}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"s\n\n# test detector model\npredictions = test(test_df, f\"{model}/{random_seed}/best/\", id2label, label2id)\n\n# results, predictions = test(test_df, f\"testing\", id2label, label2id)\n\n\n# logging.info(results)\npredictions_df = pd.DataFrame({'id': test_df.id, 'label': predictions})\npredictions_df.to_json(prediction_path, lines=True, orient='records')\n\nprint(\"Prediction Done --------------\")","metadata":{"_uuid":"210e3f7e-1d6e-4f81-8e35-cb217f0efd15","_cell_guid":"57cd5d2d-8910-47c9-9b76-8ce1453c5ef6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_2nd(test_df, model_path, id2label, label2id):\n    \n    # load tokenizer from saved model \n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    # load best model\n    model = AutoModelForSequenceClassification.from_pretrained(\n       model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id\n    )\n            \n    test_dataset = Dataset.from_pandas(test_df)\n\n    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    # create Trainer\n    trainer = Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n    # get logits from predictions and evaluate results using classification report\n    predictions = trainer.predict(tokenized_test_dataset)\n    preds = np.argmax(predictions.predictions, axis=-1)\n#     metric = evaluate.load(\"bstrai/classification_report\")\n    \n    print(preds)\n    \n    # Ensure predictions and references match in length\n    if len(preds) != len(test_df.id):\n        raise ValueError(\"Mismatch between the number of predictions and references.\")\n\n#     results = metric.compute(predictions=preds, references=test_df.id)\n    \n    # return dictionary of classification report\n    return preds","metadata":{"_uuid":"aebab9d0-27d3-4ad6-be3b-0a797a79bb6b","_cell_guid":"215fe78c-08f5-4d39-8965-bc2a7ffe113e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = pd.read_json('/kaggle/input/coling-25-task-1/multilingual_dev.jsonl', lines=True)\n\ntest_df_tem = test_df[['id','text']]\n\nsampled_df = test_df_tem\n# Sample 10% of the DataFrame\nsampled_df = test_df_tem.sample(n=int(len(test_df_tem) * 0.1), random_state=42)\n\nsampled_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Predicting for score --------------\")\n\npredictions = test_2nd(sampled_df, f\"{model}/{random_seed}/best/\", id2label, label2id)\n\nlen(predictions),len(sampled_df)\n# logging.info(results)\npredictions_df = pd.DataFrame({'id': sampled_df.id, 'label': predictions})\npredictions_df.to_json(\"score_df.jsonl\", lines=True, orient='records')\n\nprint(\"Predicting for score Done --------------\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Farmat Checker","metadata":{}},{"cell_type":"code","source":"import os\nimport argparse\nimport logging\nimport json\nimport pandas as pd\n\n\n# logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\nCOLUMNS = ['id', 'label']\n\n\ndef check_format(file_path):\n    if not os.path.exists(file_path):\n        logging.error(\"File doesnt exists: {}\".format(file_path))\n        return False\n\n    try:\n        submission = pd.read_json(file_path, lines=True)[['id', 'label']]\n    except:\n        logging.error(\"File is not a valid json file: {}\".format(file_path))\n        return False\n\n    for column in COLUMNS:\n        if submission[column].isna().any():\n            logging.error(\"NA value in file {} in column {}\".format(file_path, column))\n            return False\n\n    if not submission['label'].isin(range(0, 2)).all():\n        logging.error(\"Unknown Label in file {}\".format(file_path))\n        logging.error(\"Unique Labels in the file are {}\".format(submission['label'].unique()))\n        return False\n\n    return True\n\n\n    \npred_file_path = prediction_path \n  \n# for pred_file_path in prediction_file_path:\ncheck_result = check_format(pred_file_path)\nresult = 'Format is correct' if check_result else 'Something wrong in file format'\n#     logging.info(\"Checking file: {}. Result: {}\".format(prediction_file_path, result))\nprint(result)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Score","metadata":{}},{"cell_type":"code","source":"import logging.handlers\nimport argparse\nfrom sklearn.metrics import f1_score, accuracy_score\nimport pandas as pd\nimport sys\n# sys.path.append('.')\n# from format_checker import check_format\n\n\ndef evaluate(pred_fpath, gold_fpath):\n  \n  pred_labels = pred_fpath\n  gold_labels = gold_fpath\n\n  print(gold_labels)\n  \n  merged_df = pred_labels.merge(gold_labels, on=['id'], suffixes=('_pred', '_gold'))\n\n  print(merged_df)\n\n  macro_f1 = f1_score(merged_df['label_gold'], merged_df['label_pred'], average=\"macro\", zero_division=0)\n  micro_f1 = f1_score(merged_df['label_gold'], merged_df['label_pred'], average=\"micro\", zero_division=0)\n  accuracy = accuracy_score(merged_df['label_gold'], merged_df['label_pred'])\n  \n  return macro_f1, micro_f1, accuracy\n\n\ndef validate_files(pred_files):\n  if not check_format(pred_files):\n    logging.error('Bad format for pred file {}. Cannot score.'.format(pred_files))\n    return False\n  return True\n\n\npred_file_path = predictions_df\ngold_file_path = test_df\n\nlogging.info('Prediction file format is correct')\nmacro_f1, micro_f1, accuracy = evaluate(pred_file_path, gold_file_path)\nlogging.info(\"macro-F1={:.5f}\\tmicro-F1={:.5f}\\taccuracy={:.5f}\".format(macro_f1, micro_f1, accuracy))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"macro_f1, micro_f1, accuracy ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# train_path = '/kaggle/input/coling-25-task-1/multilingual_train.jsonl' \n\n# df = pd.read_json(train_path, lines=True)\n\n# df\n\n# df['lang'].value_counts()\n\n# df[df['lang']=='ru']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}