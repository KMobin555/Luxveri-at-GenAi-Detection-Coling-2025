{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9477189,"sourceType":"datasetVersion","datasetId":5764119},{"sourceId":9478835,"sourceType":"datasetVersion","datasetId":5765381},{"sourceId":9505903,"sourceType":"datasetVersion","datasetId":5785692},{"sourceId":9766063,"sourceType":"datasetVersion","datasetId":5700992},{"sourceId":9784947,"sourceType":"datasetVersion","datasetId":5930844}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install evaluate","metadata":{"_uuid":"2d57267c-456b-4611-bf7a-86007ea7f0ab","_cell_guid":"96620005-1fea-412c-8601-116c6cad285b","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\nimport pandas as pd\nimport evaluate\nimport numpy as np\nfrom transformers import (\n    AutoModelForSequenceClassification, TrainingArguments, Trainer,\n    DataCollatorWithPadding, AutoTokenizer, set_seed, EarlyStoppingCallback\n)\nimport os\nimport argparse\nimport torch\nfrom huggingface_hub import login\n\ndef preprocess_function(examples, **fn_kwargs):\n    return fn_kwargs['tokenizer'](examples[\"text\"], truncation=True,\n                                  # max_length=1024\n                                 )\n\ndef get_data(train_path, dev_path, test_path, random_seed):\n    \"\"\"\n    function to read dataframe with columns\n    \"\"\"\n\n    train_df = pd.read_json(train_path, lines=True)\n    val_df = pd.read_json(dev_path, lines=True)\n    test_df = pd.read_json(test_path, lines=True)\n    \n    return train_df, val_df, test_df\n\ndef compute_metrics(eval_pred):\n\n    f1_metric = evaluate.load(\"f1\")\n\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    \n    results = {}\n    results.update(f1_metric.compute(predictions=predictions, references = labels, average=\"macro\"))\n\n    return results\n\n\ndef fine_tune(train_df, valid_df, checkpoints_path, id2label, label2id, model, train_tem_args):\n    \n    torch.cuda.empty_cache()\n        \n    # pandas dataframe to huggingface Dataset\n    train_dataset = Dataset.from_pandas(train_df)\n    valid_dataset = Dataset.from_pandas(valid_df)\n    \n    cache_dir = \"./huggingface_cache\"  # Directory to store the downloaded model\n    \n    api_token = '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'\n    login(api_token)\n    \n    # get tokenizer and model from huggingface\n    tokenizer = AutoTokenizer.from_pretrained(model)    \n#     tokenizer = AutoTokenizer.from_pretrained(model, padding=\"longest\", truncation=True)\n    model = AutoModelForSequenceClassification.from_pretrained(\n       model, num_labels=len(label2id), id2label=id2label, label2id=label2id,\n       ignore_mismatched_sizes=True \n\n    )\n    \n    # Ensure padding tokens are set for GPT-2\n    tokenizer.pad_token = tokenizer.eos_token\n    model.config.pad_token_id = model.config.eos_token_id\n\n#     for param in model.parameters(): param.data = param.data.contiguous()\n    \n    # tokenize data for train/valid\n    tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n    tokenized_valid_dataset = valid_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n    \n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n#     data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pin_memory=True)\n    \n\n    # Calculate eval_steps to evaluate 3 times per epoch\n    total_train_samples = len(train_dataset)\n    batch_size = train_tem_args['train_batch']\n    steps_per_epoch = total_train_samples // batch_size\n    eval_steps = steps_per_epoch // 4  # Evaluate 3 times per epoch\n\n    training_args = TrainingArguments(\n        output_dir=checkpoints_path,\n        learning_rate=train_tem_args['lr'],\n        per_device_train_batch_size=train_tem_args['train_batch'],\n        per_device_eval_batch_size=train_tem_args['val_batch'],\n        num_train_epochs=train_tem_args['epochs'],\n        weight_decay=train_tem_args['weight_decay'],\n        save_strategy=\"steps\",  # Save based on steps\n        logging_steps=eval_steps,  # Log every eval_steps\n        evaluation_strategy=\"steps\",\n        save_total_limit=4,  # Save only the last 2 checkpoints\n        save_steps=eval_steps,  # Save model every eval_steps\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n    )\n    \n    early_stopping = EarlyStoppingCallback(\n        early_stopping_patience=5,  # Stop if no improvement after 3 evaluations\n        early_stopping_threshold=0.001  # Minimum improvement threshold\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train_dataset,\n        eval_dataset=tokenized_valid_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n        callbacks=[early_stopping],  # Add early stopping callback\n    )\n\n    trainer.train()\n\n    # save best model\n    best_model_path = os.path.join(checkpoints_path, 'best')\n    \n    if not os.path.exists(best_model_path):\n        os.makedirs(best_model_path)\n    \n    trainer.save_model(best_model_path)\n\n\ndef test(test_df, model_path, id2label, label2id):\n    \n    # load tokenizer from saved model \n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    # load best model\n    model = AutoModelForSequenceClassification.from_pretrained(\n       model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id,\n       ignore_mismatched_sizes=True \n\n    )\n    \n    # Ensure padding tokens are set for GPT-2\n    tokenizer.pad_token = tokenizer.eos_token\n    model.config.pad_token_id = model.config.eos_token_id\n    \n            \n    test_dataset = Dataset.from_pandas(test_df)\n\n    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    # create Trainer\n    trainer = Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n    # get logits from predictions and evaluate results using classification report\n    predictions = trainer.predict(tokenized_test_dataset)\n    preds = np.argmax(predictions.predictions, axis=-1)\n#     metric = evaluate.load(\"bstrai/classification_report\")\n#     results = metric.compute(predictions=preds, references=predictions.label_ids)\n    \n    # return dictionary of classification report\n    return preds","metadata":{"_uuid":"7d688dc3-5db0-45f6-8694-38c3b95a974f","_cell_guid":"3a998121-caa7-406a-85bc-9e20a19a72f9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-01T05:06:03.159166Z","iopub.execute_input":"2024-11-01T05:06:03.160422Z","iopub.status.idle":"2024-11-01T05:06:10.814884Z","shell.execute_reply.started":"2024-11-01T05:06:03.160367Z","shell.execute_reply":"2024-11-01T05:06:10.814086Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Training English","metadata":{"_uuid":"61e947e8-6f42-4225-96d8-95ba9e85cef4","_cell_guid":"07553d26-d417-4acb-af1d-3117cad045f9","trusted":true}},{"cell_type":"code","source":"!pip install -q --upgrade transformers\n!pip install -q wandb==0.16.6","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Initialize Weights & Biases (W&B) in disabled mode.\nimport torch\nimport wandb\nwandb.init(mode=\"disabled\")\n\n!rm -r /kaggle/working/\n!rm -rf ~/.cache/huggingface\n!rm -rf ~/.cache/\n\nprint(\"Training started----------------------------------------------\")\n\ntorch.cuda.empty_cache()","metadata":{"_uuid":"3d93729c-a6eb-4a0c-b242-d04096a34fc8","_cell_guid":"2e0ba763-228c-4c97-9c9f-c7730bda3a2e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-01T05:06:10.816495Z","iopub.execute_input":"2024-11-01T05:06:10.817143Z","iopub.status.idle":"2024-11-01T05:06:15.651687Z","shell.execute_reply.started":"2024-11-01T05:06:10.817105Z","shell.execute_reply":"2024-11-01T05:06:15.650513Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"rm: cannot remove '/kaggle/working/': Device or resource busy\nTraining started----------------------------------------------\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Set paths and parameters\ntrain_path = '/kaggle/input/coling-25-task-1/coling25task1/en_train.jsonl' \ndev_path = '/kaggle/input/coling-25-task-1/coling25task1/en_dev.jsonl'    \ntest_path = '/kaggle/input/coling-25-task-1/coling25task1/en_devtest_text_id_only.jsonl'   \ncheckpoints_path = '/kaggle/working/checkpoints'\n\nmodel = \"roberta-base\"\nmodel_name = '/kaggle/input/eng-ensemble-coling-final/eng_roberta-base'\n\nprediction_path = '/kaggle/working/subtask_a_pred.jsonl'\nrandom_seed = 41\n\n# Set logging and seed\n# logging.basicConfig(level=logging.INFO)\nset_seed(random_seed)\n\ntrain_tem_args = {\n    'epochs': 3,\n    'lr': 2e-5,\n    'weight_decay': 0.01,\n    'train_batch': 4,\n    'val_batch': 16,\n}\n\nid2label = {0: \"human\", 1: \"machine\"}\nlabel2id = {\"human\": 0, \"machine\": 1}\n\n\n#get data for train/dev/test sets\ntrain_df, valid_df, test_df = get_data(train_path, dev_path, test_path, random_seed)\n\nreduce_val = 0.36\n\n# Sample 10% of the DataFrame\ntrain_df = train_df.sample(n=int(len(train_df) * reduce_val), random_state=42)\nvalid_df = valid_df.sample(n=int(len(valid_df) * reduce_val), random_state=42)\n\n\ntrain_df['text_length'] = train_df['text'].apply(lambda x: len(x.split()))  # Compute text lengths\ntrain_df = train_df.sort_values(by='text_length', ascending=True).drop(columns=['text_length'])  # Sort by text length\n\n\nvalid_df['text_length'] = valid_df['text'].apply(lambda x: len(x.split()))  # Compute text lengths\nvalid_df = valid_df.sort_values(by='text_length', ascending=True).drop(columns=['text_length'])  # Sort by text length\n\n\nprint(\"data loaded--------------\")\n\n# train detector model\nfine_tune(train_df, valid_df, f\"{model}/{random_seed}\", id2label, label2id, model_name, train_tem_args)\n# fine_tune(train_df, valid_df, f\"testing\", id2label, label2id, model, train_tem_args)\n\nprint(\"Training Done --------------\")","metadata":{"_uuid":"210e3f7e-1d6e-4f81-8e35-cb217f0efd15","_cell_guid":"57cd5d2d-8910-47c9-9b76-8ce1453c5ef6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-01T05:06:15.653359Z","iopub.execute_input":"2024-11-01T05:06:15.654539Z"},"trusted":true},"outputs":[{"name":"stdout","text":"data loaded--------------\nThe token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/219876 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fd555a3a56446108e5e97635d9105f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/94232 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7e32d0d402b402c9426933e615e7338"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/tmp/ipykernel_139/1735279227.py:107: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='128' max='164907' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   128/164907 00:27 < 10:04:05, 4.55 it/s, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define your variables\ndir_to_zip = f\"{model}/{random_seed}/best/\"\n\n# Define the name of the output zip file\nlast_word = model.split('/')[-1]\noutput_zip = f\"eng_{last_word}.zip\"\n\n# Create a zip file from the directory\nshutil.make_archive(output_zip.replace('.zip', ''), 'zip', dir_to_zip)\n\nprint(f\"Zipped contents of {dir_to_zip} into {output_zip}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"s\n\ntest_df['text_length'] = test_df['text'].apply(lambda x: len(x.split()))  # Compute text lengths\ntest_df = test_df.sort_values(by='text_length', ascending=True).drop(columns=['text_length'])  # Sort by text length\n\n\n# test detector model\npredictions = test(test_df, f\"{model}/{random_seed}/best/\", id2label, label2id)\n\n\n# logging.info(results)\npredictions_df = pd.DataFrame({'id': test_df.id, 'label': predictions})\npredictions_df.to_json(prediction_path, lines=True, orient='records')\n\nprint(\"Prediction Done --------------\")","metadata":{"_uuid":"210e3f7e-1d6e-4f81-8e35-cb217f0efd15","_cell_guid":"57cd5d2d-8910-47c9-9b76-8ce1453c5ef6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_2nd(test_df, model_path, id2label, label2id):\n    \n    # load tokenizer from saved model \n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    # load best model\n    model = AutoModelForSequenceClassification.from_pretrained(\n       model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id\n    )\n            \n    test_dataset = Dataset.from_pandas(test_df)\n\n    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    # create Trainer\n    trainer = Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n    # get logits from predictions and evaluate results using classification report\n    predictions = trainer.predict(tokenized_test_dataset)\n    preds = np.argmax(predictions.predictions, axis=-1)\n#     metric = evaluate.load(\"bstrai/classification_report\")\n    \n    print(preds)\n    \n    # Ensure predictions and references match in length\n    if len(preds) != len(test_df.id):\n        raise ValueError(\"Mismatch between the number of predictions and references.\")\n\n#     results = metric.compute(predictions=preds, references=test_df.id)\n    \n    # return dictionary of classification report\n    return preds","metadata":{"_uuid":"aebab9d0-27d3-4ad6-be3b-0a797a79bb6b","_cell_guid":"215fe78c-08f5-4d39-8965-bc2a7ffe113e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = pd.read_json('/kaggle/input/coling-25-task-1/en_dev.jsonl', lines=True)\n\ntest_df = test_df.sample(n=int(len(test_df) * 0.03), random_state=42)\n\nsampled_df = test_df[['id','text']]\nsampled_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Predicting for score --------------\")\n\npredictions = test_2nd(sampled_df, f\"{model}/{random_seed}/best/\", id2label, label2id)\n\n# len(predictions),len(sampled_df)\n# logging.info(results)\npredictions_df = pd.DataFrame({'id': sampled_df.id, 'label': predictions})\npredictions_df.to_json(\"score_df.jsonl\", lines=True, orient='records')\n\nprint(\"Predicting for score Done --------------\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Farmat Checker","metadata":{}},{"cell_type":"code","source":"import os\nimport argparse\nimport logging\nimport json\nimport pandas as pd\n\n\n# logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\nCOLUMNS = ['id', 'label']\n\n\ndef check_format(file_path):\n  if not os.path.exists(file_path):\n    logging.error(\"File doesnt exists: {}\".format(file_path))\n    return False\n  \n  try:\n    submission = pd.read_json(file_path, lines=True)[['id', 'label']]\n  except:\n    logging.error(\"File is not a valid json file: {}\".format(file_path))\n    return False\n  \n  for column in COLUMNS:\n    if submission[column].isna().any():\n      logging.error(\"NA value in file {} in column {}\".format(file_path, column))\n      return False\n  \n  if not submission['label'].isin(range(0, 2)).all():\n    logging.error(\"Unknown Label in file {}\".format(file_path))\n    logging.error(\"Unique Labels in the file are {}\".format(submission['label'].unique()))\n    return False\n      \n  return True\n\n\n    \npred_file_path = prediction_path \n  \n# for pred_file_path in prediction_file_path:\ncheck_result = check_format(pred_file_path)\nresult = 'Format is correct' if check_result else 'Something wrong in file format'\n#     logging.info(\"Checking file: {}. Result: {}\".format(prediction_file_path, result))\nprint(result)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Score","metadata":{}},{"cell_type":"code","source":"import logging.handlers\nimport argparse\nfrom sklearn.metrics import f1_score, accuracy_score\nimport pandas as pd\nimport sys\n# sys.path.append('.')\n# from format_checker import check_format\n\n\ndef evaluate(pred_fpath, gold_fpath):\n  \n  pred_labels = pred_fpath\n  gold_labels = gold_fpath\n\n  print(gold_labels)\n  \n  merged_df = pred_labels.merge(gold_labels, on=['id'], suffixes=('_pred', '_gold'))\n\n  print(merged_df)\n\n  macro_f1 = f1_score(merged_df['label_gold'], merged_df['label_pred'], average=\"macro\", zero_division=0)\n  micro_f1 = f1_score(merged_df['label_gold'], merged_df['label_pred'], average=\"micro\", zero_division=0)\n  accuracy = accuracy_score(merged_df['label_gold'], merged_df['label_pred'])\n  \n  return macro_f1, micro_f1, accuracy\n\n\ndef validate_files(pred_files):\n  if not check_format(pred_files):\n    logging.error('Bad format for pred file {}. Cannot score.'.format(pred_files))\n    return False\n  return True\n\n\npred_file_path = predictions_df\ngold_file_path = test_df\n\nlogging.info('Prediction file format is correct')\nmacro_f1, micro_f1, accuracy = evaluate(pred_file_path, gold_file_path)\nlogging.info(\"macro-F1={:.5f}\\tmicro-F1={:.5f}\\taccuracy={:.5f}\".format(macro_f1, micro_f1, accuracy))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"macro_f1, micro_f1, accuracy ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}