{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "96620005-1fea-412c-8601-116c6cad285b",
    "_uuid": "2d57267c-456b-4611-bf7a-86007ea7f0ab",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip -q install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3a998121-caa7-406a-85bc-9e20a19a72f9",
    "_uuid": "7d688dc3-5db0-45f6-8694-38c3b95a974f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-01T05:06:03.160422Z",
     "iopub.status.busy": "2024-11-01T05:06:03.159166Z",
     "iopub.status.idle": "2024-11-01T05:06:10.814884Z",
     "shell.execute_reply": "2024-11-01T05:06:10.814086Z",
     "shell.execute_reply.started": "2024-11-01T05:06:03.160367Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, TrainingArguments, Trainer,\n",
    "    DataCollatorWithPadding, AutoTokenizer, set_seed, EarlyStoppingCallback\n",
    ")\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "def preprocess_function(examples, **fn_kwargs):\n",
    "    return fn_kwargs['tokenizer'](examples[\"text\"], truncation=True,\n",
    "                                  # max_length=1024\n",
    "                                 )\n",
    "\n",
    "def get_data(train_path, dev_path, test_path, random_seed):\n",
    "    \"\"\"\n",
    "    function to read dataframe with columns\n",
    "    \"\"\"\n",
    "\n",
    "    train_df = pd.read_json(train_path, lines=True)\n",
    "    val_df = pd.read_json(dev_path, lines=True)\n",
    "    test_df = pd.read_json(test_path, lines=True)\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    results = {}\n",
    "    results.update(f1_metric.compute(predictions=predictions, references = labels, average=\"macro\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def fine_tune(train_df, valid_df, checkpoints_path, id2label, label2id, model, train_tem_args):\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    # pandas dataframe to huggingface Dataset\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    valid_dataset = Dataset.from_pandas(valid_df)\n",
    "    \n",
    "    cache_dir = \"./huggingface_cache\"  # Directory to store the downloaded model\n",
    "    \n",
    "    api_token = 'use_your_own_token'\n",
    "    login(api_token)\n",
    "    \n",
    "    # get tokenizer and model from huggingface\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)    \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model, padding=\"longest\", truncation=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "       model, num_labels=len(label2id), id2label=id2label, label2id=label2id,\n",
    "       ignore_mismatched_sizes=True \n",
    "\n",
    "    )\n",
    "    \n",
    "    # Ensure padding tokens are set for GPT-2\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "#     for param in model.parameters(): param.data = param.data.contiguous()\n",
    "    \n",
    "    # tokenize data for train/valid\n",
    "    tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n",
    "    tokenized_valid_dataset = valid_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n",
    "    \n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "#     data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pin_memory=True)\n",
    "    \n",
    "\n",
    "    # Calculate eval_steps to evaluate 3 times per epoch\n",
    "    total_train_samples = len(train_dataset)\n",
    "    batch_size = train_tem_args['train_batch']\n",
    "    steps_per_epoch = total_train_samples // batch_size\n",
    "    eval_steps = steps_per_epoch // 4  # Evaluate 3 times per epoch\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=checkpoints_path,\n",
    "        learning_rate=train_tem_args['lr'],\n",
    "        per_device_train_batch_size=train_tem_args['train_batch'],\n",
    "        per_device_eval_batch_size=train_tem_args['val_batch'],\n",
    "        num_train_epochs=train_tem_args['epochs'],\n",
    "        weight_decay=train_tem_args['weight_decay'],\n",
    "        save_strategy=\"steps\",  # Save based on steps\n",
    "        logging_steps=eval_steps,  # Log every eval_steps\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_total_limit=4,  # Save only the last 2 checkpoints\n",
    "        save_steps=eval_steps,  # Save model every eval_steps\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStoppingCallback(\n",
    "        early_stopping_patience=5,  # Stop if no improvement after 3 evaluations\n",
    "        early_stopping_threshold=0.001  # Minimum improvement threshold\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_valid_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[early_stopping],  # Add early stopping callback\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # save best model\n",
    "    best_model_path = os.path.join(checkpoints_path, 'best')\n",
    "    \n",
    "    if not os.path.exists(best_model_path):\n",
    "        os.makedirs(best_model_path)\n",
    "    \n",
    "    trainer.save_model(best_model_path)\n",
    "\n",
    "\n",
    "def test(test_df, model_path, id2label, label2id):\n",
    "    \n",
    "    # load tokenizer from saved model \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # load best model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "       model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id,\n",
    "       ignore_mismatched_sizes=True \n",
    "\n",
    "    )\n",
    "    \n",
    "    # Ensure padding tokens are set for GPT-2\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "            \n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # create Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    # get logits from predictions and evaluate results using classification report\n",
    "    predictions = trainer.predict(tokenized_test_dataset)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "#     metric = evaluate.load(\"bstrai/classification_report\")\n",
    "#     results = metric.compute(predictions=preds, references=predictions.label_ids)\n",
    "    \n",
    "    # return dictionary of classification report\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "07553d26-d417-4acb-af1d-3117cad045f9",
    "_uuid": "61e947e8-6f42-4225-96d8-95ba9e85cef4",
    "trusted": true
   },
   "source": [
    "# Training English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade transformers\n",
    "!pip install -q wandb==0.16.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "2e0ba763-228c-4c97-9c9f-c7730bda3a2e",
    "_uuid": "3d93729c-a6eb-4a0c-b242-d04096a34fc8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-01T05:06:10.817143Z",
     "iopub.status.busy": "2024-11-01T05:06:10.816495Z",
     "iopub.status.idle": "2024-11-01T05:06:15.651687Z",
     "shell.execute_reply": "2024-11-01T05:06:15.650513Z",
     "shell.execute_reply.started": "2024-11-01T05:06:10.817105Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/kaggle/working/': Device or resource busy\n",
      "Training started----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# # Initialize Weights & Biases (W&B) in disabled mode.\n",
    "import torch\n",
    "import wandb\n",
    "wandb.init(mode=\"disabled\")\n",
    "\n",
    "!rm -r /kaggle/working/\n",
    "!rm -rf ~/.cache/huggingface\n",
    "!rm -rf ~/.cache/\n",
    "\n",
    "print(\"Training started----------------------------------------------\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "57cd5d2d-8910-47c9-9b76-8ce1453c5ef6",
    "_uuid": "210e3f7e-1d6e-4f81-8e35-cb217f0efd15",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-01T05:06:15.654539Z",
     "iopub.status.busy": "2024-11-01T05:06:15.653359Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded--------------\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd555a3a56446108e5e97635d9105f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/219876 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e32d0d402b402c9426933e615e7338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/94232 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_139/1735279227.py:107: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128' max='164907' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   128/164907 00:27 < 10:04:05, 4.55 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set paths and parameters\n",
    "train_path = '/kaggle/input/coling-25-task-1/coling25task1/en_train.jsonl' \n",
    "dev_path = '/kaggle/input/coling-25-task-1/coling25task1/en_dev.jsonl'    \n",
    "test_path = '/kaggle/input/coling-25-task-1/coling25task1/en_devtest_text_id_only.jsonl'   \n",
    "checkpoints_path = '/kaggle/working/checkpoints'\n",
    "\n",
    "model = \"roberta-base\"\n",
    "model_name = '/kaggle/input/eng-ensemble-coling-final/eng_roberta-base'\n",
    "\n",
    "prediction_path = '/kaggle/working/subtask_a_pred.jsonl'\n",
    "random_seed = 41\n",
    "\n",
    "# Set logging and seed\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "set_seed(random_seed)\n",
    "\n",
    "train_tem_args = {\n",
    "    'epochs': 3,\n",
    "    'lr': 2e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'train_batch': 4,\n",
    "    'val_batch': 16,\n",
    "}\n",
    "\n",
    "id2label = {0: \"human\", 1: \"machine\"}\n",
    "label2id = {\"human\": 0, \"machine\": 1}\n",
    "\n",
    "\n",
    "#get data for train/dev/test sets\n",
    "train_df, valid_df, test_df = get_data(train_path, dev_path, test_path, random_seed)\n",
    "\n",
    "reduce_val = 0.36\n",
    "\n",
    "# Sample 10% of the DataFrame\n",
    "train_df = train_df.sample(n=int(len(train_df) * reduce_val), random_state=42)\n",
    "valid_df = valid_df.sample(n=int(len(valid_df) * reduce_val), random_state=42)\n",
    "\n",
    "\n",
    "train_df['text_length'] = train_df['text'].apply(lambda x: len(x.split()))  # Compute text lengths\n",
    "train_df = train_df.sort_values(by='text_length', ascending=True).drop(columns=['text_length'])  # Sort by text length\n",
    "\n",
    "\n",
    "valid_df['text_length'] = valid_df['text'].apply(lambda x: len(x.split()))  # Compute text lengths\n",
    "valid_df = valid_df.sort_values(by='text_length', ascending=True).drop(columns=['text_length'])  # Sort by text length\n",
    "\n",
    "\n",
    "print(\"data loaded--------------\")\n",
    "\n",
    "# train detector model\n",
    "fine_tune(train_df, valid_df, f\"{model}/{random_seed}\", id2label, label2id, model_name, train_tem_args)\n",
    "# fine_tune(train_df, valid_df, f\"testing\", id2label, label2id, model, train_tem_args)\n",
    "\n",
    "print(\"Training Done --------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define your variables\n",
    "dir_to_zip = f\"{model}/{random_seed}/best/\"\n",
    "\n",
    "# Define the name of the output zip file\n",
    "last_word = model.split('/')[-1]\n",
    "output_zip = f\"eng_{last_word}.zip\"\n",
    "\n",
    "# Create a zip file from the directory\n",
    "shutil.make_archive(output_zip.replace('.zip', ''), 'zip', dir_to_zip)\n",
    "\n",
    "print(f\"Zipped contents of {dir_to_zip} into {output_zip}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "57cd5d2d-8910-47c9-9b76-8ce1453c5ef6",
    "_uuid": "210e3f7e-1d6e-4f81-8e35-cb217f0efd15",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "s\n",
    "\n",
    "test_df['text_length'] = test_df['text'].apply(lambda x: len(x.split()))  # Compute text lengths\n",
    "test_df = test_df.sort_values(by='text_length', ascending=True).drop(columns=['text_length'])  # Sort by text length\n",
    "\n",
    "\n",
    "# test detector model\n",
    "predictions = test(test_df, f\"{model}/{random_seed}/best/\", id2label, label2id)\n",
    "\n",
    "\n",
    "# logging.info(results)\n",
    "predictions_df = pd.DataFrame({'id': test_df.id, 'label': predictions})\n",
    "predictions_df.to_json(prediction_path, lines=True, orient='records')\n",
    "\n",
    "print(\"Prediction Done --------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "215fe78c-08f5-4d39-8965-bc2a7ffe113e",
    "_uuid": "aebab9d0-27d3-4ad6-be3b-0a797a79bb6b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_2nd(test_df, model_path, id2label, label2id):\n",
    "    \n",
    "    # load tokenizer from saved model \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # load best model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "       model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    "    )\n",
    "            \n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # create Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    # get logits from predictions and evaluate results using classification report\n",
    "    predictions = trainer.predict(tokenized_test_dataset)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "#     metric = evaluate.load(\"bstrai/classification_report\")\n",
    "    \n",
    "    print(preds)\n",
    "    \n",
    "    # Ensure predictions and references match in length\n",
    "    if len(preds) != len(test_df.id):\n",
    "        raise ValueError(\"Mismatch between the number of predictions and references.\")\n",
    "\n",
    "#     results = metric.compute(predictions=preds, references=test_df.id)\n",
    "    \n",
    "    # return dictionary of classification report\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_json('/kaggle/input/coling-25-task-1/en_dev.jsonl', lines=True)\n",
    "\n",
    "test_df = test_df.sample(n=int(len(test_df) * 0.03), random_state=42)\n",
    "\n",
    "sampled_df = test_df[['id','text']]\n",
    "sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Predicting for score --------------\")\n",
    "\n",
    "predictions = test_2nd(sampled_df, f\"{model}/{random_seed}/best/\", id2label, label2id)\n",
    "\n",
    "# len(predictions),len(sampled_df)\n",
    "# logging.info(results)\n",
    "predictions_df = pd.DataFrame({'id': sampled_df.id, 'label': predictions})\n",
    "predictions_df.to_json(\"score_df.jsonl\", lines=True, orient='records')\n",
    "\n",
    "print(\"Predicting for score Done --------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Farmat Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "COLUMNS = ['id', 'label']\n",
    "\n",
    "\n",
    "def check_format(file_path):\n",
    "  if not os.path.exists(file_path):\n",
    "    logging.error(\"File doesnt exists: {}\".format(file_path))\n",
    "    return False\n",
    "  \n",
    "  try:\n",
    "    submission = pd.read_json(file_path, lines=True)[['id', 'label']]\n",
    "  except:\n",
    "    logging.error(\"File is not a valid json file: {}\".format(file_path))\n",
    "    return False\n",
    "  \n",
    "  for column in COLUMNS:\n",
    "    if submission[column].isna().any():\n",
    "      logging.error(\"NA value in file {} in column {}\".format(file_path, column))\n",
    "      return False\n",
    "  \n",
    "  if not submission['label'].isin(range(0, 2)).all():\n",
    "    logging.error(\"Unknown Label in file {}\".format(file_path))\n",
    "    logging.error(\"Unique Labels in the file are {}\".format(submission['label'].unique()))\n",
    "    return False\n",
    "      \n",
    "  return True\n",
    "\n",
    "\n",
    "    \n",
    "pred_file_path = prediction_path \n",
    "  \n",
    "# for pred_file_path in prediction_file_path:\n",
    "check_result = check_format(pred_file_path)\n",
    "result = 'Format is correct' if check_result else 'Something wrong in file format'\n",
    "#     logging.info(\"Checking file: {}. Result: {}\".format(prediction_file_path, result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import logging.handlers\n",
    "import argparse\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "import sys\n",
    "# sys.path.append('.')\n",
    "# from format_checker import check_format\n",
    "\n",
    "\n",
    "def evaluate(pred_fpath, gold_fpath):\n",
    "  \n",
    "  pred_labels = pred_fpath\n",
    "  gold_labels = gold_fpath\n",
    "\n",
    "  print(gold_labels)\n",
    "  \n",
    "  merged_df = pred_labels.merge(gold_labels, on=['id'], suffixes=('_pred', '_gold'))\n",
    "\n",
    "  print(merged_df)\n",
    "\n",
    "  macro_f1 = f1_score(merged_df['label_gold'], merged_df['label_pred'], average=\"macro\", zero_division=0)\n",
    "  micro_f1 = f1_score(merged_df['label_gold'], merged_df['label_pred'], average=\"micro\", zero_division=0)\n",
    "  accuracy = accuracy_score(merged_df['label_gold'], merged_df['label_pred'])\n",
    "  \n",
    "  return macro_f1, micro_f1, accuracy\n",
    "\n",
    "\n",
    "def validate_files(pred_files):\n",
    "  if not check_format(pred_files):\n",
    "    logging.error('Bad format for pred file {}. Cannot score.'.format(pred_files))\n",
    "    return False\n",
    "  return True\n",
    "\n",
    "\n",
    "pred_file_path = predictions_df\n",
    "gold_file_path = test_df\n",
    "\n",
    "logging.info('Prediction file format is correct')\n",
    "macro_f1, micro_f1, accuracy = evaluate(pred_file_path, gold_file_path)\n",
    "logging.info(\"macro-F1={:.5f}\\tmicro-F1={:.5f}\\taccuracy={:.5f}\".format(macro_f1, micro_f1, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "macro_f1, micro_f1, accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5764119,
     "sourceId": 9477189,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5765381,
     "sourceId": 9478835,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5785692,
     "sourceId": 9505903,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5700992,
     "sourceId": 9766063,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5930844,
     "sourceId": 9784947,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
