{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9459386,"sourceType":"datasetVersion","datasetId":5750659},{"sourceId":9505903,"sourceType":"datasetVersion","datasetId":5785692},{"sourceId":9520813,"sourceType":"datasetVersion","datasetId":5794906},{"sourceId":9747083,"sourceType":"datasetVersion","datasetId":5964915},{"sourceId":9766063,"sourceType":"datasetVersion","datasetId":5700992}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q evaluate\n\n!pip install -q --upgrade transformers\n!pip install -q wandb==0.16.6","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Initialize Weights & Biases (W&B) in disabled mode.\n\nimport wandb\nwandb.init(mode=\"disabled\")\n\n!rm -r /kaggle/working/\nprint(\"Training started----------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2024-10-30T18:54:36.365628Z","iopub.execute_input":"2024-10-30T18:54:36.365992Z","iopub.status.idle":"2024-10-30T18:54:39.456334Z","shell.execute_reply.started":"2024-10-30T18:54:36.365947Z","shell.execute_reply":"2024-10-30T18:54:39.454965Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport logging\nimport pandas as pd\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, DataCollatorWithPadding\nimport numpy as np\nfrom datasets import Dataset\n\n# logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n\n\ndef preprocess_function(examples, **fn_kwargs):\n    return fn_kwargs['tokenizer'](examples[\"text\"], truncation=True ,max_length = 512)\n\n# Function to make predictions for each model\ndef predict_and_save(test_df, model_path, id2label, label2id, output_file):\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id\n    )\n    \n    test_dataset = Dataset.from_pandas(test_df)\n    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    \n    # Create Trainer\n    trainer = Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n    \n    # Get predictions\n    predictions = trainer.predict(tokenized_test_dataset)\n    preds = np.argmax(predictions.predictions, axis=-1)\n    \n    # Save predictions to JSONL\n    predictions_df = pd.DataFrame({'testset_id': test_df['testset_id'], 'label': preds})\n    predictions_df.to_json(output_file, lines=True, orient='records')\n    \n    print(f\"Predictions saved to {output_file}\")\n    return preds\n\n# Function to evaluate predictions\ndef evaluate(predictions, gold_labels):\n    merged_df = predictions.merge(gold_labels, on=['testset_id'], suffixes=('_pred', '_gold'))\n    macro_f1 = f1_score(merged_df['label_gold'], merged_df['label_pred'], average=\"macro\", zero_division=0)\n    micro_f1 = f1_score(merged_df['label_gold'], merged_df['label_pred'], average=\"micro\", zero_division=0)\n    accuracy = accuracy_score(merged_df['label_gold'], merged_df['label_pred'])\n    \n    return macro_f1, micro_f1, accuracy","metadata":{"execution":{"iopub.status.busy":"2024-10-30T18:54:39.458796Z","iopub.execute_input":"2024-10-30T18:54:39.459941Z","iopub.status.idle":"2024-10-30T18:54:59.637219Z","shell.execute_reply.started":"2024-10-30T18:54:39.459902Z","shell.execute_reply":"2024-10-30T18:54:59.636410Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Paths to your trained models\nmodel_paths = [\n    \"/kaggle/input/multi-lingual-coling-ensemble/mullang_mdeberta-v3-base_mxlen512\",\n#     \"/kaggle/input/multi-lingual-coling-ensemble/mul_lin_ens_1/mullang_bert-base-multilingual-cased\",\n#     \"/kaggle/input/multi-lingual-coling-ensemble/mul_lin_ens_1/mullang_bert-base-multilingual-uncased\",\n#     \"/kaggle/input/multi-lingual-coling-ensemble/mul_lin_ens_1/mullang_canine-c\",\n#     \"/kaggle/input/multi-lingual-coling-ensemble/mul_lin_ens_1/mullang_distilbert-base-multilingual-cased\",\n#     \"/kaggle/input/multi-lingual-coling-ensemble/mul_lin_ens_1/mullang_mdeberta-v3-base\",\n#     \"/kaggle/input/multi-lingual-coling-ensemble/mul_lin_ens_1/mullang_rembert_mxlen128\",\n#     \"/kaggle/input/multi-lingual-coling-ensemble/mul_lin_ens_1/mullang_rembert_mxlen256\",\n#     \"/kaggle/input/multi-lingual-coling-ensemble/mul_lin_ens_1/mullang_xlm-roberta-base\"\n]\n\nmodel_paths = [\n    \"/kaggle/input/mul-ensemble-coling-final/mullang_rembert\",\n#     \"/kaggle/input/mul-ensemble-coling-final/mullang_xlm-roberta-base(1)\",\n#     \"/kaggle/input/mul-ensemble-coling-final/mullang_bert-base-multilingual-cased(1)\"\n]\n\n# test_df = pd.read_json('/kaggle/input/short-mul-lang-coling/multilingual_dev_for_score.jsonl', lines=True)\ntest_df = pd.read_json('/kaggle/input/coling-25-task-1/test_set_multilingual.jsonl', lines=True)\n\ntest_df_tem = test_df\n\ntest_df = test_df.sample(n=int(len(test_df) * 0.0001), random_state=42)\n# print(test_df['label'].value_counts())\n\ntest_df_tem = test_df[['testset_id','text']]\n\n\n\n\nid2label = {0: \"human\", 1: \"machine\"}\nlabel2id = {\"human\": 0, \"machine\": 1}\n\nfile_path_op = None\n\n# Iterate over each model path\nfor model_path in model_paths:\n    model_name = os.path.basename(model_path)  # Extract model name from path\n    \n    # Generate prediction file name based on model name\n    output_file = f\"taskb_{model_name}.jsonl\"\n    file_path_op = output_file\n    \n    print(f\"\\n\\n{model_name} is predicting-----------\")\n    \n    # Make predictions and save\n    predictions = predict_and_save(test_df_tem, model_path, id2label, label2id, output_file)\n    \n    # Load predictions for evaluation\n    pred_df = pd.read_json(output_file, lines=True)\n    \n    # Evaluate predictions\n#     macro_f1, micro_f1, accuracy = evaluate(pred_df, test_df)\n    \n# #     # Log the scores\n# #     logging.info(f\"Scores for {model_name}:\")\n# #     logging.info(f\"Macro-F1: {macro_f1:.5f}, Micro-F1: {micro_f1:.5f}, Accuracy: {accuracy:.5f}\")\n    \n#     print(f\"Scores for {model_name}:\")\n#     print(f\"Macro-F1: {macro_f1:.5f}, Micro-F1: {micro_f1:.5f}, Accuracy: {accuracy:.5f}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T18:54:59.638621Z","iopub.execute_input":"2024-10-30T18:54:59.639265Z","iopub.status.idle":"2024-10-30T18:55:46.448483Z","shell.execute_reply.started":"2024-10-30T18:54:59.639223Z","shell.execute_reply":"2024-10-30T18:55:46.447572Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport argparse\nimport logging\nimport json\nimport pandas as pd\n\n\n# logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\nCOLUMNS = ['testset_id', 'label']\n\n\ndef check_format(file_path):\n    if not os.path.exists(file_path):\n        logging.error(\"File doesnt exists: {}\".format(file_path))\n        return False\n\n    try:\n        submission = pd.read_json(file_path, lines=True)[['testset_id', 'label']]\n    except:\n        logging.error(\"File is not a valid json file: {}\".format(file_path))\n        return False\n\n    for column in COLUMNS:\n        if submission[column].isna().any():\n            logging.error(\"NA value in file {} in column {}\".format(file_path, column))\n            return False\n\n    if not submission['label'].isin(range(0, 2)).all():\n        logging.error(\"Unknown Label in file {}\".format(file_path))\n        logging.error(\"Unique Labels in the file are {}\".format(submission['label'].unique()))\n        return False\n\n    return True\n\n  \ncheck_result = check_format(file_path_op)\nresult = 'Format is correct' if check_result else 'Something wrong in file format'\n#     logging.info(\"Checking file: {}. Result: {}\".format(prediction_file_path, result))\nprint(file_path_op ,\" \",result)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T18:55:46.450351Z","iopub.execute_input":"2024-10-30T18:55:46.450665Z","iopub.status.idle":"2024-10-30T18:55:46.467828Z","shell.execute_reply.started":"2024-10-30T18:55:46.450632Z","shell.execute_reply":"2024-10-30T18:55:46.466915Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ensemble","metadata":{}},{"cell_type":"code","source":"import os\nimport logging\nimport pandas as pd\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, DataCollatorWithPadding\nimport numpy as np\nfrom datasets import Dataset\n\n# Preprocessing function for tokenization\ndef preprocess_function(examples, **fn_kwargs):\n    tokenizer = fn_kwargs['tokenizer']\n    # Ensure all entries are strings and join if necessary\n    examples[\"text\"] = [' '.join(map(str, text)) if isinstance(text, list) else str(text) for text in examples[\"text\"]]\n    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n\n\n# Function to get model predictions and probabilities\ndef predict_probs(test_df, model_path, id2label, label2id):\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id\n    )\n#     model.eval()\n    \n#     test_df['text'] = test_df['text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n\n    test_dataset = Dataset.from_pandas(test_df)\n    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    \n    trainer = Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n    \n    # Get raw logits\n    predictions = trainer.predict(tokenized_test_dataset)\n    \n    # Return the logits (not softmaxed)\n    return predictions.predictions\n\n# Function to calculate perplexity from logits\ndef calculate_perplexity(logits, labels):\n    # Apply softmax to get probabilities\n    probs = np.exp(logits) / np.exp(logits).sum(axis=-1, keepdims=True)\n    \n    # Get the probabilities of the true class\n    true_probs = np.array([probs[i, label] for i, label in enumerate(labels)])\n    \n    # Calculate perplexity\n    neg_log_likelihood = -np.mean(np.log(true_probs + 1e-10))  # Add small value to prevent log(0)\n    perplexity = np.exp(neg_log_likelihood)\n    \n    return perplexity\n\n# Function to aggregate predictions using weighted average (soft voting)\ndef ensemble_predict_with_weights(test_df, model_paths, id2label, label2id, perplexities):\n    model_weights = calculate_weights(perplexities)\n    print(f\"Model weights based on perplexity: {model_weights}\")\n    \n    all_probs = None\n    \n    for i, model_path in enumerate(model_paths):\n        print(f\"\\n\\n{os.path.basename(model_path)} is predicting-----------\")\n        \n        # Get prediction logits from each model\n        model_probs = predict_probs(test_df, model_path, id2label, label2id)\n        \n        # Apply weight to the model's probabilities\n        weighted_probs = model_probs * model_weights[i]\n        \n        if all_probs is None:\n            all_probs = weighted_probs\n        else:\n            all_probs += weighted_probs  # Accumulate the weighted probabilities\n        \n#         print(f\"Weighted probs for {os.path.basename(model_path)}: {weighted_probs}\")\n    \n    avg_probs = all_probs\n    final_preds = np.argmax(avg_probs, axis=-1)\n    \n    return final_preds\n\n# Function to calculate weights based on inverse perplexity\ndef calculate_weights(perplexities):\n    inverse_perplexities = 1 / np.array(perplexities)\n    normalized_weights = inverse_perplexities / inverse_perplexities.sum()\n    return normalized_weights\n\n# Function to evaluate predictions\ndef evaluate(predictions, gold_labels):\n    merged_df = predictions.merge(gold_labels, on=['testset_id'], suffixes=('_pred', '_gold'))\n    macro_f1 = f1_score(merged_df['label_gold'], merged_df['label_pred'], average=\"macro\", zero_division=0)\n    micro_f1 = f1_score(merged_df['label_gold'], merged_df['label_pred'], average=\"micro\", zero_division=0)\n    accuracy = accuracy_score(merged_df['label_gold'], merged_df['label_pred'])\n    \n    return macro_f1, micro_f1, accuracy\n\n\n# ----------------------------------------------------------------------------------------------------------------------\n\n# Paths to your trained models\n\nmodel_paths = [\n#     \"/kaggle/input/mul-ensemble-coling-final/mullang_rembert\",\n#     \"/kaggle/input/mul-ensemble-coling-final/mullang_xlm-roberta-base(1)\",\n    \"/kaggle/input/mul-ensemble-coling-final/mullang_bert-base-multilingual-cased(1)\"\n]\n\n# Load test data\nweight_cal_df = pd.read_json('/kaggle/input/short-mul-lang-coling/multilingual_dev_for_score.jsonl', lines=True)\nweight_cal_df = weight_cal_df.sample(n=int(len(weight_cal_df) * 0.01), random_state=42)\nweight_cal_df_tem = weight_cal_df[['id', 'text']]\n\nlabels = weight_cal_df['label'].values\n\ntest_df = pd.read_json('/kaggle/input/coling-25-task-1/test_set_multilingual.jsonl', lines=True)\n# test_df = test_df.sample(n=int(len(test_df) * 0.0001), random_state=42)\ntest_df_tem = test_df[['testset_id', 'text']]\n\n# Dictionary to map labels (adjust as needed)\nid2label = {0: \"human\", 1: \"machine\"}\nlabel2id = {\"human\": 0, \"machine\": 1}\n\n# Calculate perplexities for each model\nperplexities = []\nfor model_path in model_paths:\n    print(f\"Calculating perplexity for {os.path.basename(model_path)}...\")\n    logits = predict_probs(weight_cal_df_tem, model_path, id2label, label2id)\n    perplexity = calculate_perplexity(logits, labels)\n    print(f\"Perplexity for {os.path.basename(model_path)}: {perplexity:.5f}\")\n    perplexities.append((perplexity-1))\n    \n# perplexities = [0.15131,0.15264]\n\n# Use the weighted ensemble method for predictions based on perplexity\nfinal_preds = ensemble_predict_with_weights(test_df_tem, model_paths, id2label, label2id, perplexities)\n\n# Generate a dynamic filename based on model names\ndef generate_filename(model_paths):\n    # Extract base model names from paths\n    model_names = [os.path.basename(model_path) for model_path in model_paths]\n    # Join the model names with underscores\n    filename = \"mul_ensemble_predictions.jsonl\"\n    return filename\n\n# Use the generated filename for saving predictions\nensemble_filename = generate_filename(model_paths)\nprint(\"ensemble file name\", ensemble_filename)\n\n# Save ensemble predictions to JSONL\nensemble_predictions_df = pd.DataFrame({'testset_id': test_df['testset_id'], 'label': final_preds})\nensemble_predictions_df.to_json(ensemble_filename, lines=True, orient='records')","metadata":{"execution":{"iopub.status.busy":"2024-10-30T18:55:46.469117Z","iopub.execute_input":"2024-10-30T18:55:46.469502Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport argparse\nimport logging\nimport json\nimport pandas as pd\n\n\n# logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\nCOLUMNS = ['testset_id', 'label']\n\n\ndef check_format(file_path):\n    if not os.path.exists(file_path):\n        logging.error(\"File doesnt exists: {}\".format(file_path))\n        return False\n\n    try:\n        submission = pd.read_json(file_path, lines=True)[['testset_id', 'label']]\n    except:\n        logging.error(\"File is not a valid json file: {}\".format(file_path))\n        return False\n\n    for column in COLUMNS:\n        if submission[column].isna().any():\n            logging.error(\"NA value in file {} in column {}\".format(file_path, column))\n            return False\n\n    if not submission['label'].isin(range(0, 2)).all():\n        logging.error(\"Unknown Label in file {}\".format(file_path))\n        logging.error(\"Unique Labels in the file are {}\".format(submission['label'].unique()))\n        return False\n\n    return True\n\n  \ncheck_result = check_format(ensemble_filename)\nresult = 'Format is correct' if check_result else 'Something wrong in file format'\n#     logging.info(\"Checking file: {}. Result: {}\".format(prediction_file_path, result))\nprint(ensemble_filename ,\" \",result)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Load ensemble predictions for evaluation\n# pred_df = pd.read_json(ensemble_filename, lines=True)\n\n# # Evaluate ensemble predictions\n# macro_f1, micro_f1, accuracy = evaluate(pred_df, test_df)\n\n# print(f\"Ensemble Scores:\")\n# print(f\"Macro-F1: {macro_f1:.5f}, Micro-F1: {micro_f1:.5f}, Accuracy: {accuracy:.5f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}