{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9477189,"sourceType":"datasetVersion","datasetId":5764119},{"sourceId":9478835,"sourceType":"datasetVersion","datasetId":5765381},{"sourceId":9489220,"sourceType":"datasetVersion","datasetId":5773158},{"sourceId":9715957,"sourceType":"datasetVersion","datasetId":5930844},{"sourceId":9766063,"sourceType":"datasetVersion","datasetId":5700992},{"sourceId":9769091,"sourceType":"datasetVersion","datasetId":5983353}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !cp /kaggle/input/coling-25-task-1/* /kaggle/working/","metadata":{"_uuid":"fd4d4d3b-a2e1-4283-ae29-47923bea8c95","_cell_guid":"ce596ee0-1a91-469b-b29b-40547a3cf0fc","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q evaluate\n\n!pip install -q --upgrade transformers\n!pip install -q wandb==0.16.6","metadata":{"_uuid":"09b8fca1-8f16-4a0d-9aa9-47cfa04fdd43","_cell_guid":"d33fafda-2294-4d6e-9ded-d8b0c7160d55","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Initialize Weights & Biases (W&B) in disabled mode.\nimport torch\nimport wandb\nwandb.init(mode=\"disabled\")\n\n!rm -r /kaggle/working/\n\ntorch.cuda.empty_cache()\n\nprint(\"Training started----------------------------------------------\")","metadata":{"_uuid":"dc0683a7-c23b-4673-9d5f-7b51c7b66264","_cell_guid":"15136687-c432-4b19-b8d6-70b7bb78cfe9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-30T18:04:33.866348Z","iopub.execute_input":"2024-10-30T18:04:33.867010Z","iopub.status.idle":"2024-10-30T18:04:38.427103Z","shell.execute_reply.started":"2024-10-30T18:04:33.866958Z","shell.execute_reply":"2024-10-30T18:04:38.425944Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport logging\nimport pandas as pd\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, DataCollatorWithPadding\nimport numpy as np\nfrom datasets import Dataset\n\nlogging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n\n\ndef preprocess_function(examples, **fn_kwargs):\n    return fn_kwargs['tokenizer'](examples[\"text\"], truncation=True)\n\n# Function to make predictions for each model\ndef predict_and_save(test_df, model_path, id2label, label2id, output_file):\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id\n    )\n    \n    # Add padding token if not available\n    if tokenizer.pad_token is None:\n        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n        model.resize_token_embeddings(len(tokenizer))\n    \n    test_dataset = Dataset.from_pandas(test_df)\n    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    \n    # Create Trainer\n    trainer = Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n    \n    # Get predictions\n    predictions = trainer.predict(tokenized_test_dataset)\n    preds = np.argmax(predictions.predictions, axis=-1)\n    \n    # Save predictions to JSONL\n    predictions_df = pd.DataFrame({'testset_id': test_df['testset_id'], 'label': preds})\n    predictions_df.to_json(output_file, lines=True, orient='records')\n    \n    print(f\"Predictions saved to {output_file}\")\n    return preds\n\n# Function to evaluate predictions\ndef evaluate(predictions, gold_labels):\n    merged_df = predictions.merge(gold_labels, on=['id'], suffixes=('_pred', '_gold'))\n    macro_f1 = f1_score(merged_df['label_gold'], merged_df['label_pred'], average=\"macro\", zero_division=0)\n    micro_f1 = f1_score(merged_df['label_gold'], merged_df['label_pred'], average=\"micro\", zero_division=0)\n    accuracy = accuracy_score(merged_df['label_gold'], merged_df['label_pred'])\n    \n    return macro_f1, micro_f1, accuracy","metadata":{"_uuid":"28c18c39-71a3-4819-a968-199bc371a880","_cell_guid":"626cd1b8-0d39-4aaa-9514-ff81798974f7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-30T18:04:38.430016Z","iopub.execute_input":"2024-10-30T18:04:38.430987Z","iopub.status.idle":"2024-10-30T18:04:44.690300Z","shell.execute_reply.started":"2024-10-30T18:04:38.430944Z","shell.execute_reply":"2024-10-30T18:04:44.689424Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Paths to your trained models\nmodel_paths = [\n    \"/kaggle/input/coling-ensemble-1/roberta-base/roberta-base\",\n#     \"/kaggle/input/coling-ensemble-1/bert-base-cased/bert-base-cased\",\n#     \"/kaggle/input/coling-ensemble-1/bert-base-uncased/bert-base-uncased\",\n#     \"/kaggle/input/coling-ensemble-1/distilbert-base-uncased\",\n#     \"/kaggle/input/coling-ensemble-1/distilbert-base-uncased-finetuned-sst-2-english\",\n#     \"/kaggle/input/coling-ensemble-1/distilroberta-base/distilroberta-base\",\n#     \"/kaggle/input/coling-ensemble-1/albert-base-v2/albert-base-v2\",\n#     \"/kaggle/input/coling-ensemble-1/roberta-base-openai-detector/roberta-base-openai-detector\",\n#     \"/kaggle/input/coling-ensemble-1/xlm-roberta-base/xlm-roberta-base\",\n#     \"/kaggle/input/trained-output-1/roberta-base/42/best\"\n]\n\nmodel_paths = [\n    \"/kaggle/input/eng-ensemble-coling-final/eng_roberta-base\",\n#     \"/kaggle/input/eng-ensemble-coling-final/eng_roberta-base-openai-detector\",\n#     \"/kaggle/input/eng-ensemble-coling-final/eng_albert-base-v2\",\n#     \"/kaggle/input/eng-ensemble-coling-final/eng_bert-base-cased\"\n]\n\ntest_df = pd.read_json('/kaggle/input/coling-25-task-1/test_set_en.jsonl', lines=True)\n\nnum_samples = int(len(test_df) * 0.0001)\ntest_df = test_df.sample(n=num_samples, random_state=42)\n\ntest_df_tem = test_df[['testset_id','text']]\n\n\nid2label = {0: \"human\", 1: \"machine\"}\nlabel2id = {\"human\": 0, \"machine\": 1}\n\n# Iterate over each model path\nfor model_path in model_paths:\n    model_name = os.path.basename(model_path)  # Extract model name from path\n    \n    # Generate prediction file name based on model name\n    output_file = f\"prediction_{model_name}.jsonl\"\n    \n    print(f\"\\n\\n{model_name} is predicting-----------\")\n    \n    # Make predictions and save\n    predictions = predict_and_save(test_df_tem, model_path, id2label, label2id, output_file)\n    \n    # Load predictions for evaluation\n    pred_df = pd.read_json(output_file, lines=True)\n    \n    # Evaluate predictions\n#     macro_f1, micro_f1, accuracy = evaluate(pred_df, test_df)\n    \n#     # Log the scores\n#     logging.info(f\"Scores for {model_name}:\")\n#     logging.info(f\"Macro-F1: {macro_f1:.5f}, Micro-F1: {micro_f1:.5f}, Accuracy: {accuracy:.5f}\")\n    \n#     print(f\"Scores for {model_name}:\")\n#     print(f\"Macro-F1: {macro_f1:.5f}, Micro-F1: {micro_f1:.5f}, Accuracy: {accuracy:.5f}\")\n\n","metadata":{"_uuid":"0579e078-4c2b-43a6-a127-2afae201d0d8","_cell_guid":"309fc113-ad00-47be-83ea-7b72ad4cbef4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-30T18:04:44.691531Z","iopub.execute_input":"2024-10-30T18:04:44.692145Z","iopub.status.idle":"2024-10-30T18:04:47.378928Z","shell.execute_reply.started":"2024-10-30T18:04:44.692109Z","shell.execute_reply":"2024-10-30T18:04:47.377943Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Farmat Checker","metadata":{"_uuid":"b9b4b723-01a3-4600-a7d3-9c71beb48957","_cell_guid":"eb407800-5182-4991-8303-5cb73c097737","trusted":true}},{"cell_type":"code","source":"import os\nimport argparse\nimport logging\nimport json\nimport pandas as pd\n\n\nlogging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\nCOLUMNS = ['testset_id', 'label']\n\n\ndef check_format(file_path):\n    if not os.path.exists(file_path):\n        logging.error(\"File doesnt exists: {}\".format(file_path))\n        return False\n\n    try:\n        submission = pd.read_json(file_path, lines=True)[COLUMNS]\n    except:\n        logging.error(\"File is not a valid json file: {}\".format(file_path))\n        return False\n\n    for column in COLUMNS:\n        if submission[column].isna().any():\n            logging.error(\"NA value in file {} in column {}\".format(file_path, column))\n            return False\n\n    if not submission['label'].isin(range(0, 2)).all():\n        logging.error(\"Unknown Label in file {}\".format(file_path))\n        logging.error(\"Unique Labels in the file are {}\".format(submission['label'].unique()))\n        return False\n\n    return True\n\n\n    \n# pred_file_path = '/kaggle/working/predictions.jsonl'\n\nfor model_path in model_paths:\n    model_name = os.path.basename(model_path)  # Extract model name from path\n    \n    # Generate prediction file name based on model name\n    pred_file_path = f\"prediction_{model_name}.jsonl\"\n  \n    # for pred_file_path in prediction_file_path:\n    check_result = check_format(pred_file_path)\n    result = 'Format is correct' if check_result else 'Something wrong in file format'\n    #     logging.info(\"Checking file: {}. Result: {}\".format(prediction_file_path, result))\n    print(pred_file_path ,\" \",result)","metadata":{"_uuid":"e03b3a45-9cb6-4d57-9362-16341faf6b40","_cell_guid":"13c38bfb-960e-47f7-869c-446a2a5ed338","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-30T18:04:47.380834Z","iopub.execute_input":"2024-10-30T18:04:47.381183Z","iopub.status.idle":"2024-10-30T18:04:47.395257Z","shell.execute_reply.started":"2024-10-30T18:04:47.381143Z","shell.execute_reply":"2024-10-30T18:04:47.394378Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ensemble prototype","metadata":{"_uuid":"b26fc884-2639-4584-8952-d013792c7752","_cell_guid":"470acf53-d343-401b-a30d-af2d370352d3","trusted":true}},{"cell_type":"code","source":"import os\nimport logging\nimport pandas as pd\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, DataCollatorWithPadding\nimport numpy as np\nfrom datasets import Dataset\n\n# Preprocessing function for tokenization\ndef preprocess_function(examples, **fn_kwargs):\n    return fn_kwargs['tokenizer'](examples[\"text\"], truncation=True)\n\n# Function to get model predictions and probabilities\ndef predict_probs(test_df, model_path, id2label, label2id):\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id\n    )\n    \n    # Add padding token if not available\n    if tokenizer.pad_token is None:\n        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n        model.resize_token_embeddings(len(tokenizer))\n        \n    test_df['text'] = test_df['text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n    \n    test_dataset = Dataset.from_pandas(test_df)\n    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    \n    trainer = Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n    \n    # Get raw logits\n    predictions = trainer.predict(tokenized_test_dataset)\n    \n    # Return the logits (not softmaxed)\n    return predictions.predictions\n\n# Function to calculate perplexity from logits\ndef calculate_perplexity(logits, labels):\n    # Apply softmax to get probabilities\n    probs = np.exp(logits) / np.exp(logits).sum(axis=-1, keepdims=True)\n    \n    # Get the probabilities of the true class\n    true_probs = np.array([probs[i, label] for i, label in enumerate(labels)])\n    \n    # Calculate perplexity\n    neg_log_likelihood = -np.mean(np.log(true_probs + 1e-10))  # Add small value to prevent log(0)\n    perplexity = np.exp(neg_log_likelihood)\n    \n    return perplexity\n\n# Function to aggregate predictions using weighted average (soft voting)\ndef ensemble_predict_with_weights(test_df, model_paths, id2label, label2id, perplexities):\n    model_weights = calculate_weights(perplexities)\n    print(f\"Model weights based on perplexity: {model_weights}\")\n    \n    all_probs = None\n    \n    for i, model_path in enumerate(model_paths):\n        print(f\"\\n\\n{os.path.basename(model_path)} is predicting-----------\")\n        \n        # Get prediction logits from each model\n        model_probs = predict_probs(test_df, model_path, id2label, label2id)\n        \n        # Apply weight to the model's probabilities\n        weighted_probs = model_probs * model_weights[i]\n        \n        if all_probs is None:\n            all_probs = weighted_probs\n        else:\n            all_probs += weighted_probs  # Accumulate the weighted probabilities\n        \n#         print(f\"Weighted probs for {os.path.basename(model_path)}: {weighted_probs}\")\n    \n    avg_probs = all_probs\n    final_preds = np.argmax(avg_probs, axis=-1)\n    \n    return final_preds\n\n# Function to calculate weights based on inverse perplexity\ndef calculate_weights(perplexities):\n    inverse_perplexities = 1 / np.array(perplexities)\n    normalized_weights = inverse_perplexities / inverse_perplexities.sum()\n    return normalized_weights\n\n# Function to evaluate predictions\ndef evaluate(predictions, gold_labels):\n    merged_df = predictions.merge(gold_labels, on=['id'], suffixes=('_pred', '_gold'))\n    macro_f1 = f1_score(merged_df['label_gold'], merged_df['label_pred'], average=\"macro\", zero_division=0)\n    micro_f1 = f1_score(merged_df['label_gold'], merged_df['label_pred'], average=\"micro\", zero_division=0)\n    accuracy = accuracy_score(merged_df['label_gold'], merged_df['label_pred'])\n    \n    return macro_f1, micro_f1, accuracy\n\n\n# ----------------------------------------------------------------------------------------------------------------------\n\n# Paths to your trained models\n\nmodel_paths = [\n    \"/kaggle/input/eng-ensemble-coling-final/eng_roberta-base\",\n    \"/kaggle/input/eng-ensemble-coling-final/eng_roberta-base-openai-detector\",\n#     \"/kaggle/input/eng-ensemble-coling-final/eng_albert-base-v2\",\n#     \"/kaggle/input/eng-ensemble-coling-final/eng_bert-base-cased\",\n#     \"/kaggle/input/eng-ensemble-coling-final/eng_distilroberta-base\"\n]\n\n\n# Load test data\nweight_cal_df = pd.read_json('/kaggle/input/short-coling/en_dev_for_score.jsonl', lines=True)\n# weight_cal_df = weight_cal_df.sample(n=int(len(weight_cal_df) * 0.0001), random_state=42)\nweight_cal_df_tem = weight_cal_df[['id', 'text']]\n\nlabels = weight_cal_df['label'].values\n\ntest_df = pd.read_json('/kaggle/input/coling-25-task-1/test_set_en.jsonl', lines=True)\n# test_df = test_df.sample(n=int(len(test_df) * 0.0001), random_state=42)\ntest_df_tem = test_df[['testset_id', 'text']]\n\n# Dictionary to map labels (adjust as needed)\nid2label = {0: \"human\", 1: \"machine\"}\nlabel2id = {\"human\": 0, \"machine\": 1}\n\n# Calculate perplexities for each model\nperplexities = []\nfor model_path in model_paths:\n    print(f\"Calculating perplexity for {os.path.basename(model_path)}...\")\n    logits = predict_probs(weight_cal_df_tem, model_path, id2label, label2id)\n    perplexity = calculate_perplexity(logits, labels)\n    print(f\"Perplexity for {os.path.basename(model_path)}: {perplexity:.5f}\")\n    perplexities.append((perplexity-1))\n    \nprint(perplexities)\n# Use the weighted ensemble method for predictions based on perplexity\nfinal_preds = ensemble_predict_with_weights(test_df_tem, model_paths, id2label, label2id, perplexities)\n\n# Generate a dynamic filename based on model names\ndef generate_filename(model_paths):\n    # Extract base model names from paths\n    model_names = [os.path.basename(model_path) for model_path in model_paths]\n    # Join the model names with underscores\n    filename =\"ensemble_predictions_test_phase.jsonl\"\n    return filename\n\n# Use the generated filename for saving predictions\nensemble_filename = generate_filename(model_paths)\nprint(\"ensemble file name\", ensemble_filename)\n\n# Save ensemble predictions to JSONL\nensemble_predictions_df = pd.DataFrame({'testset_id': test_df['testset_id'], 'label': final_preds})\nensemble_predictions_df.to_json(ensemble_filename, lines=True, orient='records')","metadata":{"_uuid":"8916805a-c3ed-492a-b1ee-43a2257160ff","_cell_guid":"faa6962f-1f28-4f56-b7b9-1adb17349563","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-30T18:09:54.241984Z","iopub.execute_input":"2024-10-30T18:09:54.242399Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport argparse\nimport logging\nimport json\nimport pandas as pd\n\n\n# logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\nCOLUMNS = ['testset_id', 'label']\n\n\ndef check_format(file_path):\n    if not os.path.exists(file_path):\n        logging.error(\"File doesnt exists: {}\".format(file_path))\n        return False\n\n    try:\n        submission = pd.read_json(file_path, lines=True)[COLUMNS]\n    except:\n        logging.error(\"File is not a valid json file: {}\".format(file_path))\n        return False\n\n    for column in COLUMNS:\n        if submission[column].isna().any():\n            logging.error(\"NA value in file {} in column {}\".format(file_path, column))\n            return False\n\n    if not submission['label'].isin(range(0, 2)).all():\n        logging.error(\"Unknown Label in file {}\".format(file_path))\n        logging.error(\"Unique Labels in the file are {}\".format(submission['label'].unique()))\n        return False\n\n    return True\n\n  \ncheck_result = check_format(ensemble_filename)\nresult = 'Format is correct' if check_result else 'Something wrong in file format'\n#     logging.info(\"Checking file: {}. Result: {}\".format(prediction_file_path, result))\nprint(ensemble_filename ,\" \",result)","metadata":{"_uuid":"8916805a-c3ed-492a-b1ee-43a2257160ff","_cell_guid":"faa6962f-1f28-4f56-b7b9-1adb17349563","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Load ensemble predictions for evaluation\n# pred_df = pd.read_json(ensemble_filename, lines=True)\n\n# # Evaluate ensemble predictions\n# macro_f1, micro_f1, accuracy = evaluate(pred_df, test_df)\n\n# print(f\"Ensemble Scores:\")\n# print(f\"Macro-F1: {macro_f1:.5f}, Micro-F1: {micro_f1:.5f}, Accuracy: {accuracy:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-30T18:04:55.649484Z","iopub.status.idle":"2024-10-30T18:04:55.649817Z","shell.execute_reply.started":"2024-10-30T18:04:55.649650Z","shell.execute_reply":"2024-10-30T18:04:55.649667Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_json('/kaggle/input/emni-testing-coling-suba/subtask_a.jsonl', lines=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2024-10-31T05:47:33.807879Z","iopub.execute_input":"2024-10-31T05:47:33.808764Z","iopub.status.idle":"2024-10-31T05:47:35.272125Z","shell.execute_reply.started":"2024-10-31T05:47:33.808719Z","shell.execute_reply":"2024-10-31T05:47:35.270829Z"},"trusted":true},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"       testset_id  label\n0               0      0\n1               1      1\n2               2      1\n3               3      1\n4               4      1\n...           ...    ...\n74751       74751      0\n74752       74752      1\n74753       74753      1\n74754       74754      0\n74755       74755      1\n\n[74756 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>testset_id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>74751</th>\n      <td>74751</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>74752</th>\n      <td>74752</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>74753</th>\n      <td>74753</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>74754</th>\n      <td>74754</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>74755</th>\n      <td>74755</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>74756 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"df = df.rename(columns={'testset_id': 'id'})\ndf","metadata":{"execution":{"iopub.status.busy":"2024-10-31T05:47:47.209701Z","iopub.execute_input":"2024-10-31T05:47:47.210123Z","iopub.status.idle":"2024-10-31T05:47:47.221819Z","shell.execute_reply.started":"2024-10-31T05:47:47.210068Z","shell.execute_reply":"2024-10-31T05:47:47.220733Z"},"trusted":true},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"          id  label\n0          0      0\n1          1      1\n2          2      1\n3          3      1\n4          4      1\n...      ...    ...\n74751  74751      0\n74752  74752      1\n74753  74753      1\n74754  74754      0\n74755  74755      1\n\n[74756 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>74751</th>\n      <td>74751</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>74752</th>\n      <td>74752</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>74753</th>\n      <td>74753</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>74754</th>\n      <td>74754</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>74755</th>\n      <td>74755</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>74756 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"df.to_json('tesst1.jsonl', lines=True, orient='records')","metadata":{"execution":{"iopub.status.busy":"2024-10-31T05:48:10.014105Z","iopub.execute_input":"2024-10-31T05:48:10.014683Z","iopub.status.idle":"2024-10-31T05:48:10.088583Z","shell.execute_reply.started":"2024-10-31T05:48:10.014629Z","shell.execute_reply":"2024-10-31T05:48:10.087208Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}